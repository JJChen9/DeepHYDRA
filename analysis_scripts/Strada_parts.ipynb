{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb2dc476",
   "metadata": {},
   "source": [
    "# Testing the h5 files for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66074d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load each dataset into a DataFrame\n",
    "train_x_df = pd.read_hdf('/eos/user/k/kstehle/atlas-hlt-datasets/reduced_hlt_train_set_2023_x.h5')\n",
    "val_x_df = pd.read_hdf('/eos/user/k/kstehle/atlas-hlt-datasets/reduced_hlt_val_set_2023_x.h5')\n",
    "val_y_df = pd.read_hdf('/eos/user/k/kstehle/atlas-hlt-datasets/reduced_hlt_val_set_2023_y.h5')\n",
    "test_x_df = pd.read_hdf('/eos/user/k/kstehle/atlas-hlt-datasets/reduced_hlt_test_set_2023_x.h5')\n",
    "test_y_df = pd.read_hdf('/eos/user/k/kstehle/atlas-hlt-datasets/reduced_hlt_test_set_2023_y.h5')\n",
    "\n",
    "# Display the first few rows of each DataFrame to understand their structure\n",
    "print(\"Train X Data:\")\n",
    "print(train_x_df.head())\n",
    "\n",
    "print(\"\\nValidation X Data:\")\n",
    "print(val_x_df.head())\n",
    "\n",
    "print(\"\\nValidation Y Data:\")\n",
    "print(val_y_df.head())\n",
    "\n",
    "print(\"\\nTest X Data:\")\n",
    "print(test_x_df.head())\n",
    "\n",
    "print(\"\\nTest Y Data:\")\n",
    "print(test_y_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa5704d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fced17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Ensure the index is a DatetimeIndex\n",
    "if not isinstance(train_x_df.index, pd.DatetimeIndex):\n",
    "    train_x_df.index = pd.to_datetime(train_x_df.index)\n",
    "\n",
    "# Check if the DatetimeIndex is timezone-naive or timezone-aware\n",
    "if train_x_df.index.tz is None:\n",
    "    # If timezone-naive, localize to the desired timezone\n",
    "    train_x_df.index = train_x_df.index.tz_localize('Europe/Paris')\n",
    "else:\n",
    "    # If timezone-aware, convert to the desired timezone if necessary\n",
    "    train_x_df.index = train_x_df.index.tz_convert('Europe/Paris')\n",
    "\n",
    "# Now the index (timestamps) is ready to be used directly in plotting\n",
    "timestamp = train_x_df.index\n",
    "\n",
    "# Now the timestamp is ready to be used, and it will remain the index\n",
    "train_x_df.index = timestamp  # Assign the localized/converted timestamp back to the index\n",
    "\n",
    "# Select only the median and std columns\n",
    "median_columns = [col for col in train_x_df.columns if col.startswith('m_')]\n",
    "std_columns = [col for col in train_x_df.columns if col.startswith('std_')]\n",
    "\n",
    "# Plot each median and corresponding standard deviation as a function of time\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for median_col, std_col in zip(median_columns, std_columns):\n",
    "    plt.plot(timestamp, train_x_df[median_col], label=f'{median_col}')\n",
    "    plt.fill_between(timestamp, \n",
    "                     train_x_df[median_col] - train_x_df[std_col], \n",
    "                     train_x_df[median_col] + train_x_df[std_col], \n",
    "                     alpha=0.2, label=f'{median_col} Â± {std_col}')\n",
    "\n",
    "plt.title('Medians and Standard Deviations Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), ncol=2, borderaxespad=0.)\n",
    "\n",
    "# Set x-axis major formatter to show year, month, day, hour, minute, and second\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# Optionally set major locator to improve readability (e.g., every 10 minutes)\n",
    "plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "\n",
    "# Rotate and format the x-axis dates for better readability\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f243c65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7ab677",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb61bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc12e754",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5f8e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f30d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eaaff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1613ae32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542e37c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe63478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e03ddc7b",
   "metadata": {},
   "source": [
    "# Testing the Offline detection step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d6e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cae861",
   "metadata": {},
   "outputs": [],
   "source": [
    "rack_colors = {  0: '#D81B60',\n",
    "                 1: '#1E88E5',\n",
    "                 2: '#FFC107',\n",
    "                 3: '#004D40',\n",
    "                 4: '#C43F42',\n",
    "                 5: '#6F8098',\n",
    "                 6: '#D4FC14',\n",
    "                 7: '#1CB2C5',\n",
    "                 8: '#18F964',\n",
    "                 9: '#1164B3'}\n",
    "\n",
    "SMALL_SIZE = 13\n",
    "MEDIUM_SIZE = 13\n",
    "BIGGER_SIZE = 13\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e013215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tpu_number(channel_name):\n",
    "    parameters = [int(substring) for substring in re.findall(r'\\d+', channel_name)]\n",
    "    return parameters[4]\n",
    "\n",
    "\n",
    "def get_rack_hardware_configuration(rack_number: int,\n",
    "                                        variant: str = '2018'):\n",
    "    \n",
    "    if variant == '2018':\n",
    "\n",
    "        if 44 <= rack_number <= 54:\n",
    "            return 0\n",
    "        elif 55 <= rack_number <= 63:\n",
    "            return 1\n",
    "        elif (70 <= rack_number <= 77) or\\\n",
    "                    (79 <= rack_number <= 90):\n",
    "            return 2\n",
    "        elif 16 <= rack_number <= 26:\n",
    "            return 3\n",
    "        else:\n",
    "            raise ValueError(f'Rack number {rack_number} not '\n",
    "                                f'in known nodes for variant {variant}')\n",
    "\n",
    "    # need to check this and include the rest of the years\n",
    "    if variant == '2023':\n",
    "\n",
    "        if 44 <= rack_number <= 54:\n",
    "            return 0\n",
    "        elif 55 <= rack_number <= 63:\n",
    "            return 1\n",
    "        elif (64 <= rack_number <= 77) or\\\n",
    "                    (79 <= rack_number <= 90):\n",
    "            return 2\n",
    "        elif rack_number <= 26:\n",
    "            return 3\n",
    "        else:\n",
    "            raise ValueError(f'Rack number {rack_number} not '\n",
    "                                f'in known nodes for variant {variant}')\n",
    "    \n",
    "    else:\n",
    "        raise NotImplementedError('Rack hardware configuration '\n",
    "                                    'identification not implemented '\n",
    "                                    f'for variant {variant}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f72a54",
   "metadata": {},
   "source": [
    "# Testing the train, test and val samples for 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68f41d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#variant = \"2023\"\n",
    "#hlt_data_pd = pd.read_csv('/eos/user/k/kstehle/atlas-hlt-datasets/test_set_dcm_rates_2023.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "#xlim_lower = 0\n",
    "#xlim_upper = -1\n",
    "\n",
    "#hlt_data_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa958636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebe4cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76e0b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import argparse\n",
    "import sys\n",
    "import datetime as dt\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.contrib import tzip\n",
    "from tqdm.contrib.logging import logging_redirect_tqdm\n",
    "\n",
    "sys.path.append('/eos/user/j/jhoya/DAQ/AnomalyDetection/strada/detection_combined/')\n",
    "\n",
    "\n",
    "from reduction.medianstdreducer import MedianStdReducer\n",
    "from transformer_based_detection.informers.informerrunner import InformerRunner\n",
    "from utils.anomalyregistry import JSONAnomalyRegistry\n",
    "from utils.reduceddatabuffer import ReducedDataBuffer\n",
    "from utils.exceptions import NonCriticalPredictionException\n",
    "from utils.consolesingleton import ConsoleSingleton\n",
    "\n",
    "import clustering.dbscananomalydetector\n",
    "\n",
    "importlib.reload(clustering.dbscananomalydetector)\n",
    "from clustering.dbscananomalydetector import HLTDBSCANAnomalyDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d37289",
   "metadata": {},
   "outputs": [],
   "source": [
    "hlt_data_pd = pd.read_csv('/eos/user/k/kstehle/atlas-hlt-datasets/test_set_dcm_rates_2023.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "rack_config = '2023'\n",
    "\n",
    "tpu_labels = list(hlt_data_pd.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa755a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hlt_data_pd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0086fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ce65a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant = \"2023\"\n",
    "\n",
    "\n",
    "xlim_lower = 0\n",
    "xlim_upper = -1\n",
    "    \n",
    "    \n",
    "column_names = list(hlt_data_pd.columns.values)\n",
    "print(f'Channels: {len(column_names)}')\n",
    "\n",
    "nan_amount = np.mean(np.sum(pd.isna(hlt_data_pd.to_numpy()), 1)/hlt_data_pd.shape[1])\n",
    "print(f'Mean sparsity original dataset: {100*nan_amount:.3f} %')\n",
    "\n",
    "hlt_data_pd.dropna(axis=0,\n",
    "                  thresh=50,\n",
    "                  inplace=True)\n",
    "\n",
    "hlt_data_np = hlt_data_pd.to_numpy()\n",
    "\n",
    "nan_amount = np.mean(np.sum(pd.isna(hlt_data_np), 1)/hlt_data_pd.shape[1])\n",
    "print(f'Mean sparsity preprocessed: {100*nan_amount:.3f} %')\n",
    "\n",
    "\n",
    "tpu_numbers = [get_tpu_number(label) for label in column_names]\n",
    "tpu_numbers_unique = np.array(list(set(tpu_numbers)))\n",
    "rack_numbers = np.floor_divide(tpu_numbers, 1000)\n",
    "\n",
    "hardware_configurations =[get_rack_hardware_configuration(rack_number, variant) + 1 for rack_number in rack_numbers]\n",
    "\n",
    "channel_colors = [rack_colors[configuration] for configuration in hardware_configurations]\n",
    "\n",
    "xlim_lower = 0\n",
    "xlim_upper = -1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3), dpi=300)\n",
    "\n",
    "ax.set_xlabel('Time [h]')\n",
    "ax.set_ylabel('Event Rate [Hz]')\n",
    "\n",
    "ax.set_ylim(-1, 60)\n",
    "\n",
    "ax.grid()\n",
    "\n",
    "x = np.arange(len(hlt_data_np[xlim_lower:xlim_upper, :]))*5/3600\n",
    "\n",
    "for channel in trange(hlt_data_np.shape[-1], desc='Plotting'):\n",
    "    ax.plot(x, hlt_data_np[xlim_lower:xlim_upper, channel], linewidth=1, color=channel_colors[channel])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a8daac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06afa088",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_dbscan_eps = 3\n",
    "m_dbscan_min_samples = 4\n",
    "m_dbscan_duration_threshold = 4\n",
    "\n",
    "dbscan_anomaly_detector = HLTDBSCANAnomalyDetector(tpu_labels,\n",
    "                                                   m_dbscan_eps,\n",
    "                                                   m_dbscan_min_samples,\n",
    "                                                   m_dbscan_duration_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3496a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"output\"\n",
    "json_anomaly_registry = JSONAnomalyRegistry(output_dir)\n",
    "\n",
    "dbscan_anomaly_detector.register_detection_callback(\n",
    "                        json_anomaly_registry.clustering_detection)\n",
    "timestamps = list(hlt_data_pd.index)\n",
    "hlt_data_np = hlt_data_pd.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f04763",
   "metadata": {},
   "outputs": [],
   "source": [
    "with logging_redirect_tqdm():\n",
    "    for count, (timestamp, data) in enumerate(tzip(timestamps, hlt_data_np)):\n",
    "        try:\n",
    "            dbscan_anomaly_detector.process(timestamp, data)            \n",
    "        except NonCriticalPredictionException:\n",
    "            break\n",
    "\n",
    "json_anomaly_registry.write_log_file(\"test_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aae848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f1cd63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54bd4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bedfa7fa",
   "metadata": {},
   "source": [
    "# Original code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f92369",
   "metadata": {},
   "outputs": [],
   "source": [
    "variant = \"2023\"\n",
    "dataset_df = pd.read_csv('data/hlt_data_pd_2.csv', index_col=0)\n",
    "\n",
    "xlim_lower = 0\n",
    "xlim_upper = -1\n",
    "    \n",
    "    \n",
    "column_names = list(dataset_df.columns.values)\n",
    "print(f'Channels: {len(column_names)}')\n",
    "\n",
    "nan_amount = np.mean(np.sum(pd.isna(dataset_df.to_numpy()), 1)/dataset_df.shape[1])\n",
    "print(f'Mean sparsity original dataset: {100*nan_amount:.3f} %')\n",
    "\n",
    "dataset_df.dropna(axis=0,\n",
    "                  thresh=50,\n",
    "                  inplace=True)\n",
    "\n",
    "dataset_np = dataset_df.to_numpy()\n",
    "\n",
    "nan_amount = np.mean(np.sum(pd.isna(dataset_np), 1)/dataset_df.shape[1])\n",
    "print(f'Mean sparsity preprocessed: {100*nan_amount:.3f} %')\n",
    "\n",
    "\n",
    "tpu_numbers = [get_tpu_number(label) for label in column_names]\n",
    "tpu_numbers_unique = np.array(list(set(tpu_numbers)))\n",
    "rack_numbers = np.floor_divide(tpu_numbers, 1000)\n",
    "\n",
    "hardware_configurations =[get_rack_hardware_configuration(rack_number, variant) + 1 for rack_number in rack_numbers]\n",
    "\n",
    "channel_colors = [rack_colors[configuration] for configuration in hardware_configurations]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fb0c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 3), dpi=300)\n",
    "\n",
    "ax.set_xlabel('Time [h]')\n",
    "ax.set_ylabel('Event Rate [Hz]')\n",
    "\n",
    "ax.set_ylim(-1, 60)\n",
    "\n",
    "ax.grid()\n",
    "\n",
    "x = np.arange(len(dataset_np[xlim_lower:xlim_upper, :]))*5/3600\n",
    "\n",
    "for channel in trange(dataset_np.shape[-1], desc='Plotting'):\n",
    "    ax.plot(x, dataset_np[xlim_lower:xlim_upper, channel], linewidth=1, color=channel_colors[channel])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d656b0e",
   "metadata": {},
   "source": [
    "# Starting the model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ced021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import importlib\n",
    "import argparse\n",
    "import sys\n",
    "import datetime as dt\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.contrib import tzip\n",
    "from tqdm.contrib.logging import logging_redirect_tqdm\n",
    "\n",
    "sys.path.append('/eos/user/j/jhoya/DAQ/AnomalyDetection/strada/detection_combined/')\n",
    "\n",
    "\n",
    "from reduction.medianstdreducer import MedianStdReducer\n",
    "from transformer_based_detection.informers.informerrunner import InformerRunner\n",
    "from utils.anomalyregistry import JSONAnomalyRegistry\n",
    "from utils.reduceddatabuffer import ReducedDataBuffer\n",
    "from utils.exceptions import NonCriticalPredictionException\n",
    "from utils.consolesingleton import ConsoleSingleton\n",
    "\n",
    "import clustering.dbscananomalydetector\n",
    "\n",
    "importlib.reload(clustering.dbscananomalydetector)\n",
    "from clustering.dbscananomalydetector import HLTDBSCANAnomalyDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5052cc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "hlt_data_pd = pd.read_csv('data/hlt_data_pd_2.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "rack_config = '2023'\n",
    "\n",
    "tpu_labels = list(hlt_data_pd.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1890b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpu_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a812cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hlt24_cols = [col for col in hlt_data_pd.columns if 'HLT-24' in col]\n",
    "hlt28_cols = [col for col in hlt_data_pd.columns if 'HLT-28' in col]\n",
    "hlt32_cols = [col for col in hlt_data_pd.columns if 'HLT-32:' in col]\n",
    "hlt3232_cols = [col for col in hlt_data_pd.columns if 'HLT-32_32' in col] # 0% overallocation\n",
    "hlt3240_cols = [col for col in hlt_data_pd.columns if 'HLT-32_40' in col] # 25% overallocation\n",
    "hlt3248_cols = [col for col in hlt_data_pd.columns if 'HLT-32_48' in col] # 50% overallocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3b43d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658899d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_dbscan_eps = 3\n",
    "m_dbscan_min_samples = 4\n",
    "m_dbscan_duration_threshold = 4\n",
    "\n",
    "dbscan_anomaly_detector = HLTDBSCANAnomalyDetector(tpu_labels,\n",
    "                                                   m_dbscan_eps,\n",
    "                                                   m_dbscan_min_samples,\n",
    "                                                   m_dbscan_duration_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f30ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"output\"\n",
    "json_anomaly_registry = JSONAnomalyRegistry(output_dir)\n",
    "\n",
    "dbscan_anomaly_detector.register_detection_callback(\n",
    "                        json_anomaly_registry.clustering_detection)\n",
    "timestamps = list(hlt_data_pd.index)\n",
    "hlt_data_np = hlt_data_pd.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e782822e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aef5c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with logging_redirect_tqdm():\n",
    "    for count, (timestamp, data) in enumerate(tzip(timestamps, hlt_data_np)):\n",
    "        try:\n",
    "            dbscan_anomaly_detector.process(timestamp, data)            \n",
    "        except NonCriticalPredictionException:\n",
    "            break\n",
    "\n",
    "json_anomaly_registry.write_log_file(\"run_456522\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4707d2de",
   "metadata": {},
   "source": [
    "# Testing only on rack-48 as this was the one showing the anomalies\n",
    "Spoiler. It was a bug, there are no anomalies there..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15330c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_columns(column_name):\n",
    "    # Split the column name on \".\"\n",
    "    parts = column_name.split('.')\n",
    "    if \"rack-48\" in parts[0]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "dataset_df = pd.read_csv('data/hlt_data_pd_2.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "# Apply the filter to get the columns we want\n",
    "filtered_columns = [col for col in dataset_df.columns[:] if filter_columns(col)]\n",
    "\n",
    "filtered_df = dataset_df[filtered_columns]\n",
    "\n",
    "#print(filtered_df.shape)\n",
    "#print(filtered_df.columns)\n",
    "#print(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73394d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_label(column_name):\n",
    "    return column_name.split(':')[-1].split('.')[0].split('-')[-1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6), dpi=300)\n",
    "\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Values')\n",
    "ax.set_title('Time Series Data')\n",
    "ax.set_ylim(-1, 30)\n",
    "ax.grid()\n",
    "\n",
    "# Plot each column\n",
    "for column in filtered_df.columns:\n",
    "    label = extract_label(column)\n",
    "    ax.plot(filtered_df.index, filtered_df[column], label=label)\n",
    "\n",
    "ax.xaxis_date()\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), ncol=2, borderaxespad=0.)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97505ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628788f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# Load JSON anomaly data\n",
    "with open('output/run_456522_old.json', 'r') as file: # This was the output file when there was a bug in the tpu_labels due to the overallocation % in 2023\n",
    "    anomaly_data = json.load(file)\n",
    "\n",
    "    \n",
    "# Ensure the DataFrame index is timezone-aware\n",
    "dataset_df.index = dataset_df.index.tz_convert('Europe/Berlin')\n",
    "\n",
    "\n",
    "# Function to extract intervals for machine \"48002\"\n",
    "def extract_intervals_for_machine(anomaly_data, machine_name):\n",
    "    intervals = []\n",
    "    if machine_name in anomaly_data:\n",
    "        for timestamp_str, details in anomaly_data[machine_name].items():\n",
    "            start_time = pd.to_datetime(timestamp_str)\n",
    "            if start_time.tzinfo is None:\n",
    "                start_time = start_time.tz_localize('Europe/Berlin')\n",
    "            else:\n",
    "                start_time = start_time.tz_convert('Europe/Berlin')\n",
    "            duration = details['duration']\n",
    "            end_time = start_time + timedelta(seconds=duration)\n",
    "            intervals.append((start_time, end_time))\n",
    "    return intervals\n",
    "\n",
    "\n",
    "\n",
    "# Extract intervals for machine \"48002\"\n",
    "intervals = extract_intervals_for_machine(anomaly_data, \"48002\")\n",
    "\n",
    "# Plotting the time series data\n",
    "fig, ax = plt.subplots(figsize=(12, 6), dpi=300)\n",
    "\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Values')\n",
    "ax.set_title('Time Series Data')\n",
    "ax.set_ylim(-1, 30)\n",
    "ax.grid()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to extract the label from the column name\n",
    "def extract_label(column_name):\n",
    "    return column_name.split(':')[-1].split('.')[0].split('-')[-1]\n",
    "\n",
    "\n",
    "# Plot each column\n",
    "for column in filtered_df.columns:\n",
    "    if \"48002\" not in column: continue\n",
    "    label = extract_label(column)\n",
    "    ax.plot(filtered_df.index, filtered_df[column], label=label)\n",
    "\n",
    "ax.xaxis_date()\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "# Overlay unique anomalies\n",
    "for start, end in intervals:\n",
    "    ax.axvspan(start, end, color='red', alpha=0.5, zorder=1)\n",
    "\n",
    "    \n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), ncol=2, borderaxespad=0.)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff962a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbbcd92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac7055f1",
   "metadata": {},
   "source": [
    "# Now the proper version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c932b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('output/run_454322.json', 'r') as file: \n",
    "    anomaly_data = json.load(file)\n",
    "\n",
    "dataset_df = pd.read_csv('data/hlt_data_pd_454322.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f947399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the DataFrame index is timezone-aware\n",
    "dataset_df.index = dataset_df.index.tz_convert('Europe/Berlin')  \n",
    "# Function to extract intervals for all machines\n",
    "def extract_intervals_for_all_machines(anomaly_data):\n",
    "    intervals = {}\n",
    "    for machine_name, anomalies in anomaly_data.items():\n",
    "        intervals[machine_name] = []\n",
    "        for timestamp_str, details in anomalies.items():\n",
    "            start_time = pd.to_datetime(timestamp_str)\n",
    "            if start_time.tzinfo is None:\n",
    "                start_time = start_time.tz_localize('Europe/Berlin')\n",
    "            else:\n",
    "                start_time = start_time.tz_convert('Europe/Berlin')\n",
    "            duration = details['duration'] * 5  # Convert timesteps to seconds (each timestep is 5 seconds)\n",
    "            end_time = start_time + timedelta(seconds=duration)\n",
    "            anomaly_type = details['types'][0]\n",
    "            intervals[machine_name].append((start_time, end_time, anomaly_type))\n",
    "    return intervals\n",
    "\n",
    "\n",
    "# Extract intervals for all machines\n",
    "intervals = extract_intervals_for_all_machines(anomaly_data)\n",
    "\n",
    "# Plotting the time series data for all machines with anomalies\n",
    "fig, ax = plt.subplots(figsize=(12, 6), dpi=300)\n",
    "\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Values')\n",
    "ax.set_title('Time Series Data')\n",
    "ax.set_ylim(-1, 30)\n",
    "ax.grid()\n",
    "\n",
    "# Function to extract the label from the column name\n",
    "def extract_label(column_name):\n",
    "    return column_name.split(':')[-1].split('.')[0].split('-')[-1]\n",
    "\n",
    "# Initialize legend entries to keep track of added anomaly types\n",
    "legend_entries = set()\n",
    "\n",
    "# Plot each column for each machine with anomalies\n",
    "for machine_id, machine_intervals in intervals.items():\n",
    "    machine_columns = [col for col in dataset_df.columns if str(machine_id) in col]\n",
    "    for column in machine_columns:\n",
    "        label = extract_label(column)\n",
    "        ax.plot(dataset_df.index, dataset_df[column], label=f'Node {label}')\n",
    "        \n",
    "    # Overlay unique anomalies using axvspan\n",
    "    for start, end, anomaly_type in machine_intervals:\n",
    "        color = 'red' if anomaly_type == 'ClusteringDropToZero' else 'blue'\n",
    "        if anomaly_type not in legend_entries:\n",
    "            ax.axvspan(start, end, color=color, alpha=0.5, zorder=1, label=anomaly_type)\n",
    "            legend_entries.add(anomaly_type)\n",
    "        else:\n",
    "            ax.axvspan(start, end, color=color, alpha=0.5, zorder=1)\n",
    "            \n",
    "        # Add text annotation for machine_id\n",
    "        ax.text(start, ax.get_ylim()[1] - 1, f'{machine_id}', rotation=90, verticalalignment='top', fontsize=8, color=color)\n",
    "\n",
    "        \n",
    "\n",
    "# Improve the formatting of the x-axis for dates\n",
    "ax.xaxis_date()\n",
    "fig.autofmt_xdate()\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), ncol=1, borderaxespad=0.)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d3f910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4479564f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8c7f32a",
   "metadata": {},
   "source": [
    "# Getting the plots for all the runs with anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097ef634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.dates as mdates\n",
    "import random\n",
    "import os\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "# Function to extract intervals for all machines\n",
    "def extract_intervals_for_all_machines(anomaly_data):\n",
    "    intervals = {}\n",
    "    for machine_name, anomalies in anomaly_data.items():\n",
    "        machine_name = machine_name.zfill(5)  # Ensure machine ID has 5 digits\n",
    "        intervals[machine_name] = []\n",
    "        for timestamp_str, details in anomalies.items():\n",
    "            start_time = pd.to_datetime(timestamp_str)\n",
    "            if start_time.tzinfo is None:\n",
    "                start_time = start_time.tz_localize('Europe/Berlin')\n",
    "            else:\n",
    "                start_time = start_time.tz_convert('Europe/Berlin')\n",
    "            duration = details['duration'] * 5  # Convert timesteps to seconds (each timestep is 5 seconds)\n",
    "            end_time = start_time + timedelta(seconds=duration)\n",
    "            anomaly_type = details['types'][0]\n",
    "            intervals[machine_name].append((start_time, end_time, anomaly_type))\n",
    "    return intervals\n",
    "\n",
    "# Function to extract the label from the column name\n",
    "def extract_label(column_name):\n",
    "    return column_name.split(':')[-1].split('.')[0].split('-')[-1]\n",
    "\n",
    "# Define a color palette for different machines\n",
    "colors = list(mcolors.TABLEAU_COLORS.values())\n",
    "\n",
    "def lighten_color(color, amount=0.5):\n",
    "    \"\"\"\n",
    "    Lightens the given color by mixing it with white.\n",
    "    \n",
    "    Parameters:\n",
    "    color (str or tuple): Matplotlib color\n",
    "    amount (float): Amount to lighten the color (0 to 1)\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Lightened color\n",
    "    \"\"\"\n",
    "    try:\n",
    "        c = mcolors.cnames[color]\n",
    "    except:\n",
    "        c = color\n",
    "    c = mcolors.to_rgba(c)\n",
    "    white = np.array([1, 1, 1, 1])\n",
    "    return tuple((1 - amount) * np.array(c) + amount * white)\n",
    "\n",
    "def plot_run(run_number, pdf=None):\n",
    "    # Load JSON anomaly data\n",
    "    json_file = f'output/run_{run_number}.json'\n",
    "    csv_file = f'data/hlt_data_pd_{run_number}.csv'\n",
    "\n",
    "    if not os.path.exists(json_file) or not os.path.exists(csv_file):\n",
    "        print(f\"Missing files for run number {run_number}\")\n",
    "        return\n",
    "\n",
    "    with open(json_file, 'r') as file: \n",
    "        anomaly_data = json.load(file)\n",
    "\n",
    "\n",
    "    dataset_df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
    "    # Ensure the DataFrame index is timezone-aware\n",
    "    dataset_df.index = dataset_df.index.tz_convert('Europe/Berlin') \n",
    "    \n",
    "    # Extract intervals for all machines\n",
    "    intervals = extract_intervals_for_all_machines(anomaly_data)\n",
    "\n",
    "    # Plotting the time series data for all machines with anomalies\n",
    "    fig, ax = plt.subplots(figsize=(12, 6), dpi=300)\n",
    "\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('DCM rate [kHz]')\n",
    "    ax.set_title(f'Time Series Data for Run {run_number}')\n",
    "    ax.set_ylim(-1, 60)\n",
    "    ax.grid()\n",
    "\n",
    "    legend_entries = set()\n",
    "\n",
    "    # Plot each column for each machine with anomalies and an extra machine\n",
    "    for idx, (machine_id, machine_intervals) in enumerate(intervals.items()):\n",
    "        color = colors[idx % len(colors)]\n",
    "        machine_columns = [col for col in dataset_df.columns if str(machine_id) in col]\n",
    "        \n",
    "        # Plot the actual machine with anomalies\n",
    "        for column in machine_columns:\n",
    "            label = extract_label(column)\n",
    "            ax.plot(dataset_df.index, dataset_df[column], label=f'Node - {label}', color=color)\n",
    "        \n",
    "        # Find an extra machine with the same starting number but not in the JSON file   \n",
    "        machine_prefix = str(machine_id)[:2]\n",
    "        possible_extra_machines = [col for col in dataset_df.columns if f\"tpu-rack-{machine_prefix}\" in col and str(machine_id) not in col]\n",
    "        if possible_extra_machines:\n",
    "            extra_column = random.choice(possible_extra_machines)\n",
    "            label = extract_label(extra_column)\n",
    "            ax.plot(dataset_df.index, dataset_df[extra_column], label=f'Node {label} (extra)', color=lighten_color(color, 0.5))\n",
    "\n",
    "        # Overlay unique anomalies\n",
    "        for start, end, anomaly_type in machine_intervals:\n",
    "            anomaly_color = 'red' if anomaly_type == 'ClusteringDropToZero' else 'blue'\n",
    "            if anomaly_type not in legend_entries:\n",
    "                ax.axvspan(start, end, color=anomaly_color, alpha=0.5, zorder=1, label=anomaly_type)\n",
    "                legend_entries.add(anomaly_type)\n",
    "            else:\n",
    "                ax.axvspan(start, end, color=anomaly_color, alpha=0.5, zorder=1)\n",
    "                \n",
    "            # Add text annotation for machine_id\n",
    "            ax.text(start, ax.get_ylim()[1] - 1, f'{machine_id}', rotation=90, verticalalignment='top', fontsize=8, color=color)\n",
    "\n",
    "\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d %H:%M'))\n",
    "    fig.autofmt_xdate()\n",
    "\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), ncol=1, borderaxespad=0.)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if pdf:\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def plot_zoomed_anomalies(dataset_df, intervals, run_number, pdf=None):\n",
    "    # Define the zoom window duration (e.g., 5 minutes before and after the anomaly)\n",
    "    zoom_window = timedelta(minutes=5)\n",
    "    \n",
    "    for machine_id, machine_intervals in intervals.items():\n",
    "        color = colors[hash(machine_id) % len(colors)]\n",
    "        machine_columns = [col for col in dataset_df.columns if str(machine_id) in col]\n",
    "        \n",
    "        for start, end, anomaly_type in machine_intervals:\n",
    "            fig, ax = plt.subplots(figsize=(12, 6), dpi=300)\n",
    "            ax.set_xlabel('Time')\n",
    "            ax.set_ylabel('DCM rate [kHz]')\n",
    "            ax.set_title(f'Zoomed Anomaly for Run {run_number} - Node {machine_id}')\n",
    "            ax.set_ylim(-1, 60)\n",
    "            ax.grid()\n",
    "            \n",
    "            zoom_start = start - zoom_window\n",
    "            zoom_end = end + zoom_window\n",
    "            \n",
    "            # Plot the machine's data within the zoom window\n",
    "            for column in machine_columns:\n",
    "                label = extract_label(column)\n",
    "                zoomed_data = dataset_df.loc[zoom_start:zoom_end, column]\n",
    "                ax.plot(zoomed_data.index, zoomed_data, label=f'Node - {label}', color=color)\n",
    "            \n",
    "            # Find an extra machine with the same starting number but not in the JSON file\n",
    "            machine_prefix = str(machine_id)[:2]\n",
    "            possible_extra_machines = [col for col in dataset_df.columns if f\"tpu-rack-{machine_prefix}\" in col and str(machine_id) not in col]\n",
    "            if possible_extra_machines:\n",
    "                extra_column = random.choice(possible_extra_machines)\n",
    "                label = extract_label(extra_column)\n",
    "                zoomed_data = dataset_df.loc[zoom_start:zoom_end, extra_column]\n",
    "                ax.plot(zoomed_data.index, zoomed_data, label=f'Node {label} (extra)', color=lighten_color(color, 0.5))\n",
    "\n",
    "            # Overlay the anomaly interval\n",
    "            ax.axvspan(start, end, color='red' if anomaly_type == 'ClusteringDropToZero' else 'blue', alpha=0.5, zorder=1, label=anomaly_type)\n",
    "            ax.text(start, ax.get_ylim()[1] - 1, f'{machine_id}', rotation=90, verticalalignment='top', fontsize=8, color=color)\n",
    "            \n",
    "\n",
    "            ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d %H:%M'))\n",
    "            fig.autofmt_xdate()\n",
    "\n",
    "            plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), ncol=1, borderaxespad=0.)\n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "            if pdf:\n",
    "                pdf.savefig(fig)\n",
    "                plt.close(fig)\n",
    "            else:\n",
    "                plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d168c2e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_numbers = ['454322', '455795', '455818', '456151', '456164', '456225', '456273', '456314', '455623']\n",
    "\n",
    "# for run_number in run_numbers:\n",
    "#     plot_run(run_number)\n",
    "#     # Load anomaly data and dataset_df as before\n",
    "#     json_file = f'output/run_{run_number}.json'\n",
    "#     csv_file = f'data/hlt_data_pd_{run_number}.csv'\n",
    "#     if os.path.exists(json_file) and os.path.exists(csv_file):\n",
    "#         with open(json_file, 'r') as file: \n",
    "#             anomaly_data = json.load(file)\n",
    "#         dataset_df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
    "#         dataset_df.index = dataset_df.index.tz_convert('Europe/Berlin') \n",
    "#         intervals = extract_intervals_for_all_machines(anomaly_data)\n",
    "#         plot_zoomed_anomalies(dataset_df, intervals, run_number)\n",
    "        \n",
    "        \n",
    "        \n",
    "# Create a PDF file to save all plots\n",
    "with PdfPages('anomaly_plots.pdf') as pdf:\n",
    "    for run_number in run_numbers:\n",
    "        plot_run(run_number, pdf)\n",
    "        # Load anomaly data and dataset_df as before\n",
    "        json_file = f'output/run_{run_number}.json'\n",
    "        csv_file = f'data/hlt_data_pd_{run_number}.csv'\n",
    "        if os.path.exists(json_file) and os.path.exists(csv_file):\n",
    "            with open(json_file, 'r') as file: \n",
    "                anomaly_data = json.load(file)\n",
    "            dataset_df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
    "            dataset_df.index = dataset_df.index.tz_convert('Europe/Berlin')\n",
    "            intervals = extract_intervals_for_all_machines(anomaly_data)\n",
    "            plot_zoomed_anomalies(dataset_df, intervals, run_number, pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2364eb2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6485ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run_numbers = ['456314']\n",
    "\n",
    "for run_number in run_numbers:\n",
    "    plot_run(run_number)\n",
    "    # Load anomaly data and dataset_df as before\n",
    "    json_file = f'output/run_{run_number}.json'\n",
    "    csv_file = f'data/hlt_data_pd_{run_number}.csv'\n",
    "    if os.path.exists(json_file) and os.path.exists(csv_file):\n",
    "        with open(json_file, 'r') as file: \n",
    "            anomaly_data = json.load(file)\n",
    "        dataset_df = pd.read_csv(csv_file, index_col=0, parse_dates=True)\n",
    "        dataset_df.index = dataset_df.index.tz_convert('Europe/Berlin')\n",
    "        intervals = extract_intervals_for_all_machines(anomaly_data)\n",
    "        plot_zoomed_anomalies(dataset_df, intervals, run_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc03b475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be59661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea43770",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c2134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1a8582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b672e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c67992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485d4755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa59f7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229260fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10af363d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ccac61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8cb1b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a57488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01eb401",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94be14f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333794f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa70333a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89042a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8211d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "364ed776",
   "metadata": {},
   "source": [
    "# Looking at the informer part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cdee109",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import importlib\n",
    "import argparse\n",
    "import sys\n",
    "import datetime as dt\n",
    "import json\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tqdm.contrib import tzip\n",
    "from tqdm.contrib.logging import logging_redirect_tqdm\n",
    "\n",
    "sys.path.append('/eos/user/j/jhoya/DAQ/AnomalyDetection/strada/detection_combined/')\n",
    "\n",
    "from utils.anomalyregistry import JSONAnomalyRegistry\n",
    "from utils.reduceddatabuffer import ReducedDataBuffer\n",
    "from utils.exceptions import NonCriticalPredictionException\n",
    "from utils.consolesingleton import ConsoleSingleton\n",
    "\n",
    "import clustering.dbscananomalydetector\n",
    "importlib.reload(clustering.dbscananomalydetector)\n",
    "from clustering.dbscananomalydetector import HLTDBSCANAnomalyDetector\n",
    "\n",
    "import reduction.basereducer\n",
    "importlib.reload(reduction.basereducer)\n",
    "from reduction.basereducer import BaseReducer\n",
    "\n",
    "import reduction.medianstdreducer\n",
    "importlib.reload(reduction.basereducer)\n",
    "from reduction.medianstdreducer import MedianStdReducer\n",
    "\n",
    "\n",
    "import transformer_based_detection.informers.informerrunner\n",
    "importlib.reload(transformer_based_detection.informers.informerrunner)\n",
    "from transformer_based_detection.informers.informerrunner import InformerRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76e3352e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/sft.cern.ch/lcg/views/LCG_105a_swan/x86_64-el9-gcc13-opt/lib/python3.9/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator MinMaxScaler from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#hlt_data_pd = pd.read_csv('data/hlt_data_pd_456522.csv', index_col=0, parse_dates=True)\n",
    "hlt_data_pd = pd.read_csv('data/hlt_data_pd_455795.csv', index_col=0, parse_dates=True)\n",
    "#run_numbers = ['454322', '455795', '455818', '456151', '456164', '456225', '456273', '456314', '455623']\n",
    "# Ensure the DataFrame index is timezone-aware\n",
    "hlt_data_pd.index = hlt_data_pd.index.tz_convert('Europe/Berlin') \n",
    "\n",
    "rack_config = '2023'\n",
    "\n",
    "median_std_reducer = MedianStdReducer(rack_config)\n",
    "     \n",
    "informer_runner = InformerRunner('/eos/user/k/kstehle/Documents/phd/deephydra_models/hlt_2023_mse_Scale_0.8_1.0_Scale_APP_0.8_1.0_0.01_0.05_0.05_rel_size_1.0_ratio_0.25_seed_192/', device='cpu')\n",
    "\n",
    "tpu_labels = list(hlt_data_pd.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a943930",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"output\"\n",
    "model = \"Informer-MSE\"\n",
    "\n",
    "if model == 'Informer-SMSE':\n",
    "    reduced_data_buffer = ReducedDataBuffer(size=65)\n",
    "else:\n",
    "    reduced_data_buffer = ReducedDataBuffer(size=17)\n",
    "\n",
    "\n",
    "reduced_data_buffer.set_buffer_filled_callback(informer_runner.detect)\n",
    "        \n",
    "json_anomaly_registry = JSONAnomalyRegistry(output_dir)\n",
    "\n",
    "informer_runner.register_detection_callback(\n",
    "                        json_anomaly_registry.transformer_detection)\n",
    "\n",
    "timestamps = list(hlt_data_pd.index)\n",
    "hlt_data_np = hlt_data_pd.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e32c5119",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d237935508a4ef7ad66b5b07c5e13a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10994 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some Racks are missing... 2nd stage detection performance might be affected.\n",
      "Half or more of the TPUs in rack(s) 11, 22 are inactive. Second stage detection might be affected\n",
      "Transformer-based detection encountered anomaly at timestamp 2023-07-01 16:46:00\n"
     ]
    }
   ],
   "source": [
    "with logging_redirect_tqdm():\n",
    "    for count, (timestamp, data) in enumerate(tzip(timestamps, hlt_data_np)):\n",
    "        try:\n",
    "            output_slice = median_std_reducer.reduce_numpy(tpu_labels,\n",
    "                                                            timestamp,\n",
    "                                                            data)\n",
    "            reduced_data_buffer.push(output_slice)\n",
    "        except NonCriticalPredictionException:\n",
    "            break\n",
    "\n",
    "json_anomaly_registry.write_log_file(\"run_455795\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82220283",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4826b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af83ddca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1fcfa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63be30cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41833145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577ae06c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cc8e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4655c9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7517f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1735b509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6451a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4b9eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c112d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bb6287",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
